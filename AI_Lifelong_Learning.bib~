Automatically generated by Mendeley Desktop 1.13.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Wang2011,
abstract = {Because manual image annotation is both expensive and labor intensive, in practice we often do not have sufficient labeled images to train an effective classifier for the new image classification tasks. Although multiple labeled image data sets are publicly available for a number of computer vision tasks, a simple mixture of them cannot achieve good performance due to the heterogeneous properties and structures between different data sets. In this paper, we propose a novel nonnegative matrix tri-factorization based transfer learning framework, called as Dyadic Knowledge Transfer (DKT) approach, to transfer cross-domain image knowledge for the new computer vision tasks, such as classifications. An efficient iterative algorithm to solve the proposed optimization problem is introduced. We perform the proposed approach on two benchmark image data sets to simulate the real world cross-domain image classification tasks. Promising experimental results demonstrate the effectiveness of the proposed approach.},
author = {Wang, Hua and Nie, Feiping and Huang, Heng and Ding, Chris},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126287},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2011\_wang\_Dyadic transfer learning for cross-domain image classification.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
pages = {551--556},
title = {{Dyadic transfer learning for cross-domain image classification}},
year = {2011}
}
@article{Lu2014,
author = {Lu, Zhongqi and Zhu, Yin and Pan, Sinno Jialin and Xiang, Evan Wei and Wang, Yujing and Yang, Qiang},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_Lu\_find\_source\_task\_SourceFreeTransferLearningforTextClassification.pdf:pdf},
journal = {Association for the Advancement of Artificial Intelligence},
title = {{Source Free Transfer Learning for Text Classification}},
year = {2014}
}
@article{Zhou,
author = {Zhou, Joey Tianyi and Pan, Sinno Jialin and Tsang, Ivor W and Yan, Yan},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/Deep transfer/2014\_zhou\_deep\_transfer.pdf:pdf},
journal = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
keywords = {Novel Machine Learning Algorithms},
pages = {2213--2219},
title = {{Hybrid Heterogeneous Transfer Learning through Deep Learning}},
year = {2014}
}
@inproceedings{Guyon2011,
abstract = {We organized a data mining challenge in “unsupervised and transfer learning” (the UTL challenge), in collaboration with the DARPA Deep Learning program. The goal of this year's challenge was to learn good data representations that can be re-used across tasks by building models that capture regularities of the input space. The representations provided by the participants were evaluated by the organizers on supervised learning “target tasks”, which were unknown to the participants. In a first phase of the challenge, the competitors were given only unlabeled data to learn their data representation. In a second phase of the challenge, the competitors were also provided with a limited amount of labeled data from “source tasks”, distinct from the “target tasks”. We made available large datasets from various application domains: handwriting recognition, image recognition, video processing, text processing, and ecology. The results indicate that learned data representation yield results significantly better than what can be achieved with raw data or data preprocessed with standard normalizations and functional transforms. The UTL challenge is part of the IJCNN 2011 competition program1. The website of the challenge remains open for submission of new methods beyond the termination of the challenge as a resource for students and researchers2.},
author = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Aha, David W},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2011.6033302},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2011\_guyon\_transfer\_learning\_challenge\_etal\_ijcnn11.pdf:pdf},
isbn = {9781457710865},
issn = {2161-4393},
pages = {793--800},
title = {{Unsupervised and transfer learning challenge}},
volume = {94708},
year = {2011}
}
@article{Caruana1997,
author = {Caruana, Rich},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/Deep transfer/1997\_caruana-multitask\_learning.pdf:pdf},
journal = {Macine Learning},
keywords = {backpropagation,generalization,inductive transfer,k-nearest neighbor,kernel,multitask learning,parallel transfer,regression,supervised learning},
pages = {41--75},
publisher = {Kluwer Academic Publishers, Boston. Manufactured in The Netherlands},
title = {{Multitask Learning}},
volume = {28},
year = {1997}
}
@inproceedings{Maurer2013,
abstract = {We investigate the use of sparse coding and dictionary learning in the context of multitask and transfer learning. The central assumption of our learning method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Numerical experiments on one synthetic and two real datasets show the advantage of our method over single task learning, a previous method based on orthogonal and dense representation of the tasks and a related method learning task grouping.},
archivePrefix = {arXiv},
arxivId = {arXiv:1209.0738v2},
author = {Maurer, Andreas and Pontil, Massimiliano and Romera-paredes, Bernardino},
booktitle = {ICML},
eprint = {arXiv:1209.0738v2},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2013\_mauer\_sparse\_coding\_for\_transfer.pdf:pdf},
keywords = {I,boring formatting information,machine learning},
pages = {343--351},
title = {{Sparse coding for multitask and transfer learning}},
volume = {28},
year = {2013}
}
@article{Lin2009,
abstract = {We study key issues related to multilingual acoustic modeling for automatic speech recognition (ASR) through a series of large-scale ASR experiments. Our study explores shared structures embedded in a large collection of speech data spanning over a number of spoken languages in order to establish a common set of universal phone models that can be used for large vocabulary ASR of all the languages seen or unseen during training. Language-universal and language-adaptive models are compared with language-specific models, and the comparison results show that in many cases it is possible to build general-purpose language-universal and language-adaptive acoustic models that outperform language-specific ones if the set of shared units, the structure of shared states, and the shared acoustic-phonetic properties among different languages can be properly utilized. Specifically, our results demonstrate that when the context coverage is poor in language-specific training, we can use one tenth of the adaptation data to achieve equivalent performance in cross-lingual speech recognition.},
author = {Lin, Hui and Deng, Li and Yu, Dong and Gong, Yi Fan and Acero, Alex and Lee, Chin Hui},
doi = {10.1109/ICASSP.2009.4960588},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2013\_lin\_multilingual\_0004333.pdf:pdf},
isbn = {9781424423545},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Acoustic modeling,Language adaptation,Multilingualism,Universal phone models},
pages = {4333--4336},
title = {{A study on multilingual acoustic modeling for large vocabulary ASR}},
year = {2009}
}
@article{Parameswaran2010,
abstract = {Multi-task learning (MTL) improves the prediction performance on multiple, different but related, learning problems through shared parameters or representations. One of the most prominent multi-task learning algorithms is an extension to svms by Evgeniou et al. Although very elegant, multi-task svm is inherently restricted by the fact that support vector machines require each class to be addressed explicitly with its own weight vector which, in a multi-task setting, requires the different learning tasks to share the same set of classes. This paper proposes an alternative formulation for multi-task learning by extending the recently published large margin nearest neighbor (lmnn) algorithm to the MTL paradigm. Instead of relying on separating hyperplanes, its decision function is based on the nearest neighbor rule which inherently extends to many classes and becomes a natural fit for multitask learning. We evaluate the resulting multi-task lmnn on real-world insurance data and speech classification problems and show that it consistently outperforms single-task kNN under several metrics and state-of-the-art MTL classifiers.},
author = {Parameswaran, Shibin and Weinberger, Kilian Q},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/Deep transfer/2010\_parameswaran\_large-margin-multi-task-metric-learning\_nips.pdf:pdf},
isbn = {9781617823800},
journal = {Nips},
pages = {1--9},
title = {{Large Margin Multi-Task Metric Learning}},
year = {2010}
}
@article{Seltzer2013,
author = {Seltzer, Michael L. and Droppo, Jasha},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/Deep transfer/2013\_seltzer\_multi-task\_phoneme.pdf:pdf},
isbn = {9781479903566},
journal = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
pages = {6965--6969},
title = {{MULTI-TASK LEARNING IN DEEP NEURAL NETWORKS FOR IMPROVED PHONEME RECOGNITION Michael L . Seltzer and Jasha Droppo Microsoft Research}},
year = {2013}
}
@article{Alsharif2014,
author = {Alsharif, Ouais and Bachman, Philip and Pineau, Joelle},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_Alsharif\_supplemental\_material.pdf:pdf},
pages = {1--4},
title = {{Supplemental Material for Representation as a Service}},
year = {2014}
}
@article{Zhu,
abstract = {Transfer learning as a new machine learning paradigm has gained increasing attention lately. In situations where the training data in a target domain are not sufficient to learn predictive models effectively, transfer learning leverages aux- iliary source data from other related auxiliary domains for learning. While most of the existing works in this area are only focused on using the source data with the same repre- sentational structure as the target data, in this paper, we push this boundary further by extending a heterogeneous transfer learning framework for knowledge transfer between text and images. We observe that for a target-domain classification problem, some annotated images can be found on many social Web sites, which can serve as a bridge to transfer knowledge from the abundant text documents available over the Web. A key question is how to effectively transfer the knowledge in the source data even though the text documents are arbi- trary. Our solution is to enrich the representation of the target images with semantic concepts extracted from the auxiliary source data through matrix factorization, and to use the la- tent semantic features generated by the auxiliary data to build a better image classifier. We empirically verify the effective- ness of our algorithm on the Caltech-256 image dataset.},
author = {Zhu, Yin and Chen, Yuqiang and Lu, Zhongqi},
doi = {10.1007/s00247-003-1118-z},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2011\_zhu\_Heterogeneous transfer learning for image classificationHTL\_AAAI11.pdf:pdf},
journal = {Artificial Intelligence},
pages = {1304--1309},
title = {{Heterogeneous Transfer Learning for Image Classification}},
url = {http://www.cs.ust.hk/~qyang/Docs/2011/HTL\_AAAI11.pdf},
year = {2011}
}
@article{Heigold2013,
author = {Heigold, G. and Vanhoucke, V. and Senior, a. and Nguyen, P. and Ranzato, M. and Devin, M. and Dean, J.},
doi = {10.1109/ICASSP.2013.6639348},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2013\_heigold\_icassp2013.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {15206149},
journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
pages = {8619--8623},
title = {{Multilingual acoustic models using distributed deep neural networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6639348},
year = {2013}
}
@article{Hoffman2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1407.5035v1},
author = {Hoffman, Judy and Guadarrama, Sergio and Tzeng, Eric and Donahue, Jeff and Girshick, Ross and Darrell, Trevor and Saenko, Kate},
eprint = {arXiv:1407.5035v1},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_hoffman\_LSDA\_largescale\_detection\_through\_adaptation.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {3536--3544},
title = {{LSDA: Large Scale Detection Through Adaptation}},
year = {2014}
}
@inproceedings{Ruvolo2013,
annote = {ELLA},
author = {Ruvolo, Paul and Eaton, Eric},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2013\_Easton\_ELLA.pdf:pdf},
pages = {507--515},
title = {{ELLA: An efficient lifelong learning algorithm}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013\_ruvolo13},
volume = {28},
year = {2013}
}
@article{Ammar2014,
author = {Ammar, HB and EDU, U and Eaton, Eric and Ruvolo, Paul},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_BouAmmar\_Online.pdf:pdf},
journal = {seas.upenn.edu},
title = {{Online Multi-Task Learning for Policy Gradient Methods}},
year = {2014}
}
@inproceedings{Bengio2009,
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illus- trates gradually more concepts, and gradu- ally more complex ones. Here, we formal- ize such training strategies in the context of machine learning, and call them “curricu- lum learning”. In the context of recent re- search studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neu- ral networks), we explore curriculum learn- ing in various set-ups. The experiments show that significant improvements in generaliza- tion can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
booktitle = {Proceedings of the 26th annual international conference on machine learning},
doi = {10.1145/1553374.1553380},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/Deep transfer/2009\_bengio\_curriculum\_icml.pdf:pdf},
isbn = {9781605585161},
pages = {41--48},
pmid = {5414602},
title = {{Curriculum learning}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
year = {2009}
}
@article{Daitch2009,
author = {Daitch, Samuel I. and Kelner, Jonathan a. and Spielman, Daniel a.},
doi = {10.1145/1553374.1553400},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2009\_daitch\_graph2data\_icml\_final.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
keywords = {clustering,learning on graphs,transductive classification,transductive regression},
pages = {1--8},
title = {{Fitting a graph to vector data}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553400},
year = {2009}
}
@article{Thrun1996,
abstract = {This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks. 1},
author = {Thrun, Sebastian},
doi = {10.1.1.44.2898},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/Deep transfer/1996\_thrun\_lifelong\_learning.pdf:pdf},
issn = {1049-5258},
journal = {Advances in neural information processing systems},
pages = {640--646},
title = {{Is learning the n-th thing any easier than learning the first?}},
url = {http://aima.eecs.berkeley.edu/~russell/classes/cs294/f05/papers/thrun-1996.pdf$\backslash$npapers3://publication/uuid/01414200-662C-47BC-901D-40C15253E17F},
year = {1996}
}
@article{Wang2014b,
author = {Wang, Xuezhi and Schneider, Jeff},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_wang\_transfer\_distributions\_5632-flexible-transfer-learning-under-support-and-model-shift.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1898--1906},
title = {{Flexible Transfer Learning under Support and Model Shift}},
year = {2014}
}
@article{Li2014,
abstract = {We propose an heterogeneous multi-task learning framework for human pose estimation from monocular image with deep convolutional neural network. In particular, we simultaneously learn a pose-joint regressor and a sliding-window body-part detector in a deep network architecture. We show that including the body-part detection task helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several data sets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts.},
archivePrefix = {arXiv},
arxivId = {1406.3474},
author = {Li, Sijin and Liu, Zhi-Qiang and Chan, Antoni B},
doi = {10.1109/CVPRW.2014.78},
eprint = {1406.3474},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/Deep transfer/2014\_Li\_heterogeneousHumanPose\_CVPR.pdf:pdf},
isbn = {9781479943081},
issn = {15731405},
journal = {CVPR},
title = {{Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network}},
url = {http://arxiv.org/abs/1406.3474},
year = {2014}
}
@article{Mi2014,
annote = {*bio},
author = {Mi, Yuanyuan and Wang, Dahui and Wu, Si},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_mi\_bio\_persistent\_activity\_nips\_a-synaptical-story-of-persistent-activity-with-graded-lifetime-in-a-neural-system.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {352--360},
title = {{A Synaptical Story of Persistent Activity with Graded Lifetime in a Neural System}},
year = {2014}
}
@article{Girshick2013a,
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/neural\_nets/2013\_Girschick\_RCNN.pdf:pdf},
journal = {Computer Vision and Pattern Recognition (CVPR)},
month = nov,
pages = {580--587},
publisher = {IEEE},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.org/abs/1311.2524v5},
year = {2014}
}
@article{Fang2014,
annote = {*active learning with a touch of transfer},
author = {Fang, Meng and Yin, Jie and Tao, Dacheng},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_fang\_transfer\_to\_estimate\_labels.pdf:pdf},
isbn = {9781577356790},
journal = {Twenty-Eighth AAAI Conference on Artificial Intelligence},
keywords = {Novel Machine Learning Algorithms},
pages = {1809--1815},
title = {{Active Learning for Crowdsourcing Using Knowledge Transfer}},
year = {2014}
}
@inproceedings{Bengio2011,
abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher- level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution P(x) is structurally related to some task of interest, say predicting P(y|x). This paper focusses on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution},
author = {Bengio, Yoshua},
booktitle = {JMLR: Workshop and Conference Proceedings 7},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/neural\_nets/2012\_bengio\_deep\_learning\_transfer.pdf:pdf},
keywords = {autoencoders,deep learning,domain adaptation,ing,multi-task learning,neural networks,re-,representation learning,self-taught learning,stricted boltzmann machines,transfer learn-,unsupervised learning},
pages = {1--20},
title = {{Deep Learning of Representations for Unsupervised and Transfer Learning}},
volume = {7},
year = {2011}
}
@article{Chen2014a,
author = {Chen, Zhiyuan and Liu, Bing},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_chen\_lifelong\_topic\_modeling\_icml.pdf:pdf},
isbn = {9781634393973},
journal = {Icml},
pages = {703--711},
title = {{Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data}},
volume = {32},
year = {2014}
}
@inproceedings{Raina2007,
abstract = {We present a new machine learning frame- work called “self-taught learning” for using unlabeled data in supervised classification tasks. We do not assume that the unla- beled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly eas- ier to obtain than in typical semi-supervised or transfer learning settings, making self- taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level fea- tures using the unlabeled data. These fea- tures form a succinct input representation and significantly improve classification per- formance. When using an SVM for classifi- cation, we further show how a Fisher kernel can be learned for this representation.},
author = {Raina, Rajat and Battle, Alexis and Lee, Honglak and Packer, Benjamin and Ng, Andrew Y},
booktitle = {Proceedings of the 24th International Conference on Machine Learning (2007)},
doi = {10.1145/1273496.1273592},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2007\_Raina\_selftaughtlearning\_icml07.pdf:pdf},
isbn = {9781595937933},
pages = {759--766},
title = {{Self-taught Learning : Transfer Learning from Unlabeled Data}},
year = {2007}
}
@article{Pan2010a,
author = {Pan, Sinno Jialin and Yang, Qiang},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2010\_pan\_transfer\_learning\_survey.pdf:pdf},
journal = {Knowledge and Data Engineering, IEEE Transactions on},
number = {10},
title = {{A Survey on Transfer Learning}},
volume = {22},
year = {2010}
}
@article{Deng2013a,
abstract = {Deep learning is becoming a mainstream technology for speech recognition at industrial scale. In this paper, we provide an overview of the work by Microsoft speech researchers since 2009 in this area, focusing on more recent advances which shed light to the basic capabilities and limitations of the current deep learning technology. We organize this overview along the feature-domain and model-domain dimensions according to the conventional approach to analyzing speech systems. Selected experimental results, including speech recognition and related applications such as spoken dialogue and language modeling, are presented to demonstrate and analyze the strengths and weaknesses of the techniques described in the paper. Potential improvement of these techniques and future research directions are discussed.},
author = {Deng, Li and Li, Jinyu and Huang, Jui Ting and Yao, Kaisheng and Yu, Dong and Seide, Frank and Seltzer, Michael and Zweig, Geoff and He, Xiaodong and Williams, Jason and Gong, Yifan and Acero, Alex},
doi = {10.1109/ICASSP.2013.6639345},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2013\_deng\_ICASSP-2013-OverviewMSRDeepLearning.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {convolution,deep learning,dialogue,multilingual,neural network,spectral features,speech recognition},
pages = {8604--8608},
title = {{Recent advances in deep learning for speech research at Microsoft}},
year = {2013}
}
@inproceedings{Ruvolo2013a,
author = {Ruvolo, Paul and Eaton, Eric},
booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2013\_Eaton\_ActiveTaskSelection.pdf:pdf},
isbn = {9781577356158},
keywords = {Technical Track},
pages = {862--868},
title = {{Active Task Selection for Lifelong Machine Learning}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/viewFile/6463/7285},
year = {2013}
}
@article{Chen2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1404.4108v2},
author = {Alsharif, Ouais and Bachman, Philip and Pineau, Joelle},
eprint = {arXiv:1404.4108v2},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_Alsharif\_LifelongDeepNet.pdf:pdf},
number = {1},
pages = {1--22},
title = {{Representation as a Service}},
url = {http://philsci-archive.pitt.edu/view/confandvol/confandvol20136thmstcmdm1012april2013.html},
year = {2014}
}
@article{Dahl2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1406.1231v1},
author = {Dahl, George E},
eprint = {arXiv:1406.1231v1},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_dahl\_multitask.pdf:pdf},
journal = {arXiv preprint arXiv},
pages = {1--21},
title = {{Multi-task Neural Networks for QSAR Predictions}},
year = {2014}
}
@article{Al-shedivat2014,
author = {Al-shedivat, Maruan and Wang, Jim Jing-yan and Alzahrani, Majed and Huang, Jianhua Z and Gao, Xin},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_al-shedivat\_small\_number\_labels\_transfer.pdf:pdf},
journal = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
keywords = {Novel Machine Learning Algorithms},
pages = {1665--1672},
title = {{Supervised Transfer Sparse Coding}},
year = {2014}
}
@article{DaumeIII2006,
abstract = {The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the $\backslash$in-domain" test data is drawn from a distribution that is related, but not identical, to the $\backslash$out-of-domain" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classiers and their linear chain counterparts. We present ecient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four dierent data sets from the natural language processing domain},
author = {{Daume III}, Hal and Marcu, Daniel},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2006\_daume\_domain\_adaptation\_06megam.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
keywords = {Algorithm,Algorithms,Entropy,Expectation Maximization,Training,domain adaptation,learning,performance,training data},
pages = {101--126},
title = {{Domain Adaptation for Statistical Classiers}},
url = {http://www.isi.edu/~marcu/papers/daume06megam.pdf},
volume = {26},
year = {2006}
}
@inproceedings{Zhong2012a,
abstract = {Traditionally, multitask learning (MTL) assumes that all the tasks are related. This can lead to negative transfer when tasks are indeed incoherent. Recently, a number of approaches have been proposed that alleviate this problem by discovering the underlying task clusters or relationships. However, they are limited to modeling these relationships at the task level, which may be restrictive in some applications. In this paper, we propose a novel MTL formulation that captures task relationships at the feature-level. Depending on the interactions among tasks and features, the proposed method construct different task clusters for different features, without even the need of pre-specifying the number of clusters. Computationally, the proposed formulation is strongly convex, and can be efficiently solved by accelerated proximal methods. Experiments are performed on a number of synthetic and real-world data sets. Under various degrees of task relationships, the accuracy of the proposed method is consistently among the best. Moreover, the feature-specific task clusters obtained agree with the known/plausible task structures of the data. Copyright 2012 by the author(s)/owner(s).},
annote = {decomposes w into u+v},
author = {Zhong, Leon Wenliang and Kwok, James T.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2012\_zhong\_multitask\_clustering.pdf:pdf},
isbn = {9781450312851},
pages = {49--56},
title = {{Convex multitask learning with flexible task clusters}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84867129866\&partnerID=tZOtx3y1},
volume = {1},
year = {2012}
}
@article{Bickel2008,
author = {Bickel, Steffen and Sawade, Christoph and Scheffer, Tobias},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2008\_bikel\_distribution\_nips2008\_transfer.pdf:pdf},
journal = {Nips},
pages = {145--152},
title = {{Transfer Learning by Distribution Matching for Targeted Advertising}},
year = {2008}
}
@article{Salakhutdinov2012b,
abstract = {We introduce HD (or “Hierarchical-Deep”) models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian models. Specifically we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a Deep Boltzmann Machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training examples, by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets.},
author = {Salakhutdinov, Ruslan and Tenenbaum, Joshua B and Torralba, Antonio},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2012\_salakhutdinov\_learning-to-learn-with-compound-hd-models.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 26 (NIPS 2012)},
pages = {1--9},
title = {{Learning to Learn with Compound HD Models}},
year = {2012}
}
@article{Wang2014,
author = {Wang, Xuezhi and Huang, Tzu-kuo and Schneider, Jeff},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_wang\_active\_transfer\_i14.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
pages = {1305--1313},
title = {{Active Transfer Learning under Model Shift}},
volume = {32},
year = {2014}
}
@article{Glorot2011a,
abstract = {The exponential increase in the availability of online reviews and recommendations makes sentiment classi cation an interesting topic in academic and industrial research. Reviews can span so many di erent domains that it is dicult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classi ers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classi ers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains.},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2011\_glorot\_domain\_adaptation\_icmlpaper.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {Proceedings of the 28th International Conference on Machine Learning},
number = {1},
pages = {513--520},
title = {{Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach}},
url = {http://www.icml-2011.org/papers/342\_icmlpaper.pdf},
year = {2011}
}
@article{Salakhutdinov2012,
author = {Salakhutdinov, Ruslan and Tenenbaum, Josh and Torralba, Antonio},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2012\_salakhutdinov\_oneshot12a.pdf:pdf},
journal = {JMLR W\&CP},
pages = {195--207},
title = {{One-shot learning with a hierarchical nonparametric bayesian model}},
url = {http://dspace.mit.edu/handle/1721.1/60025},
volume = {27},
year = {2012}
}
@article{Yosinski2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.1792v1},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {arXiv:1411.1792v1},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/neural\_nets/2014\_yosinkski\_deep\_transfer\_nips1411.1792v1.pdf:pdf},
journal = {In Advances in Neural Information Processing Systems},
title = {{How transferable are features in deep neural networks ?}},
volume = {27},
year = {2014}
}
@article{Deng2013,
author = {Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/Deep transfer/2013\_deng\_new\_types\_of\_learning\_ICASSP-2013-DengHintonKingsbury-revised.pdf:pdf},
isbn = {9781479903566},
journal = {ICASSP},
pages = {8599--8603},
title = {{NEW TYPES OF DEEP NEURAL NETWORK LEARNING FOR SPEECH RECOGNITION AND RELATED APPLICATIONS : AN OVERVIEW}},
year = {2013}
}
@article{Blitzer2006,
abstract = {Discriminative learning methods are widely used in natural language process- ing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource- rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our tech- nique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.},
author = {Blitzer, John and McDonald, Ryan and Pereira, Fernando},
doi = {10.3115/1610075.1610094},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2006\_blitzer\_SCL.pdf:pdf},
isbn = {1-932432-73-6},
journal = {Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006)},
pages = {120--128},
title = {{Domain adaptation with structural correspondence learning}},
url = {http://dl.acm.org/citation.cfm?id=1610094$\backslash$nhttp://dl.acm.org/citation.cfm?id=1610075.1610094},
year = {2006}
}
@inproceedings{Zhang2014,
author = {Zhang, Zhanpeng and Luo, Ping and Loy, Chen-Change and Tang, Xiaoou},
booktitle = {ECCV},
doi = {10.1007/978-3-319-10599-4\_7},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/Deep transfer/2014\_zhang\_eccv\_2014\_deepfacealign.pdf:pdf},
isbn = {978-3-319-10598-7},
title = {{Facial Landmark Detection by Deep Multi-task Learning}},
year = {2014}
}
@inproceedings{Palatucci2009,
abstract = {We consider the problem of zero-shot learning, where the goal is to$\backslash$nlearn a clas- sifier f : X -> Y that must predict novel values of$\backslash$nY that were omitted from the training set. To achieve this, we define$\backslash$nthe notion of a semantic output code classifier (SOC) which utilizes$\backslash$na knowledge base of semantic properties of Y to extrapolate to novel$\backslash$nclasses. We provide a formalism for this type of classifier and study$\backslash$nits theoretical properties in a PAC framework, showing conditions$\backslash$nun- der which the classifier can accurately predict novel classes.$\backslash$nAs a case study, we build a SOC classifier for a neural decoding$\backslash$ntask and show that it can often predict words that people are thinking$\backslash$nabout from functional magnetic resonance images (fMRI) of their neural$\backslash$nactivity, even without training examples for those words.},
author = {Palatucci, Mark and Hinton, Geoffrey and Pomerleau, Dean and Mitchell, Tom M},
booktitle = {Neural Information Processing Systems},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/20xx\_palatucci\_zero-shot-learning.pdf:pdf},
isbn = {9781615679119},
issn = {<null>},
pages = {1--9},
title = {{Zero-Shot Learning with Semantic Output Codes}},
year = {2009}
}
@article{Razavian2014,
abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the $\backslash$overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the $\backslash$overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the $\backslash$overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or \$L2\$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1403.6382},
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
eprint = {1403.6382},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/neural\_nets/2014\_Razavian\_CNN\_features.pdf:pdf},
journal = {Computer Vision and Pattern Recognition Workshops (CVPRW)},
month = mar,
pages = {512--519},
publisher = {IEEE},
title = {{CNN Features off-the-shelf: an Astounding Baseline for Recognition}},
url = {http://arxiv.org/abs/1403.6382},
year = {2014}
}
@article{Pan2013,
abstract = {A major challenge for collaborative filtering (CF) techniques in recommender systems is the data sparsity that is caused by missing and noisy ratings. This problem is even more serious for CF domains where the ratings are expressed numerically, e.g. as 5-star grades. We assume the 5-star ratings are unordered bins instead of ordinal relative preferences. We observe that, while we may lack the information in numerical ratings, we sometimes have additional auxiliary data in the form of binary ratings. This is especially true given that users can easily express themselves with their preferences expressed as likes or dislikes for items. In this paper, we explore how to use these binary auxiliary preference data to help reduce the impact of data sparsity for CF domains expressed in numerical ratings. We solve this problem by transferring the rating knowledge from some auxiliary data source in binary form (that is, likes or dislikes), to a target numerical rating matrix. In particular, our solution is to model both the numerical ratings and ratings expressed as like or dislike in a principled way. We present a novel framework of Transfer by Collective Factorization (TCF), in which we construct a shared latent space collectively and learn the data-dependent effect separately. A major advantage of the TCF approach over the previous bilinear method of collective matrix factorization is that we are able to capture the data-dependent effect when sharing the data-independent knowledge. This allows us to increase the overall quality of knowledge transfer. We present extensive experimental results to demonstrate the effectiveness of TCF at various sparsity levels, and show improvements of our approach as compared to several state-of-the-art methods. © 2013 Elsevier B.V.},
author = {Pan, Weike and Yang, Qiang},
doi = {10.1016/j.artint.2013.01.003},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2013\_Pan\_transfer\_colloborative\_filtering.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Collaborative filtering,Missing ratings,Transfer learning},
pages = {39--55},
publisher = {Elsevier B.V.},
title = {{Transfer learning in heterogeneous collaborative filtering domains}},
url = {http://dx.doi.org/10.1016/j.artint.2013.01.003},
volume = {197},
year = {2013}
}
@article{Ding2014,
author = {Ding, Zhengming and Shao, Ming and Fu, Yun},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_ding\_missing\_data\_transfer\_8256-38279-1-PB.pdf:pdf},
journal = {Twenty-Eighth AAAI Conference on Artificial Intelligence},
keywords = {Machine Learning Applications},
pages = {1192--1198},
title = {{Latent Low-Rank Transfer Subspace Learning for Missing Modality Recognition}},
year = {2014}
}
@article{Collobert2008,
abstract = {We describe a single convolutional neural net- work architecture that, given a sentence, out- puts a host of language processing predic- tions: part-of-speech tags, chunks, named en- tity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semanti- cally) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data ex- cept the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the general- ization of the shared tasks, resulting in state- of-the-art performance.},
author = {Collobert, Ronan and Weston, Jason},
doi = {10.1145/1390156.1390177},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/Deep transfer/2008\_collobert\_multitask\_nlp\_icml.pdf:pdf},
isbn = {9781605582054},
issn = {07224028},
journal = {Proceedings of the 25th International Confer- ence on Machine Learning},
pages = {160--167},
title = {{A Unified Architecture for Natural Language Processing : Deep Neural Networks with Multitask Learning}},
url = {http://portal.acm.org/citation.cfm?id=1390177},
volume = {20},
year = {2008}
}
@article{Socher2013,
author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew Y},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2013\_SocherGanjooManningNg\_zeroshot\_NIPS2013.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {935--943},
title = {{Zero-Shot Learning Through Cross-Modal Transfer}},
year = {2013}
}
@article{Blitzer2007,
abstract = {Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30\% over the original SCL algorithm and 46\% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.},
author = {Blitzer, J. and Blitzer, J. and Dredze, M. and Dredze, M. and Pereira, F. and Pereira, F.},
doi = {10.1109/IRPS.2011.5784441},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2007\_blitzer\_domain\_adaptation\_sentiment\_acl07.pdf:pdf},
isbn = {9781424491131},
issn = {0736587X},
journal = {Annual Meeting-Association for Computational Linguistics},
pages = {440},
title = {{Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Biographies,+Bollywood,+Boom-boxes+and+Blenders:+Domain+Adaptation+for+Sentiment+Classification\#0},
volume = {45},
year = {2007}
}
@inproceedings{Ciresan2012,
abstract = {We analyze transfer learning with Deep Neural Networks (DNN) on various character recognition tasks. DNN trained on digits are perfectly capable of recognizing uppercase letters with minimal retraining. They are on par with DNN fully trained on uppercase letters, but train much faster. DNN trained on Chinese characters easily recognize uppercase Latin letters. Learning Chinese characters is accelerated by first pretraining a DNN on a small subset of all classes and then continuing to train on all classes. Furthermore, pretrained nets consistently outperform randomly initialized nets on new tasks with few labeled data.},
author = {Ciresan, Dan and Meier, Ueli and Schmidhuber, Jurgen},
booktitle = {Neural Networks (IJCNN), The 2012 International Joint Conference on},
doi = {10.1109/IJCNN.2012.6252544},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/neural\_nets/2012\_ciresan\_transfer\_characters.pdf:pdf},
isbn = {978-1-4673-1490-9},
issn = {2161-4393},
pages = {1--6},
title = {{Transfer Learning for Latin and Chinese Characters with Deep Neural Networks}},
year = {2012}
}
@article{Wang2014a,
author = {Wang, Qifan and Ruan, Lingyun and Si, Luo},
file = {:C$\backslash$:/Users/alienbot/Documents/papers/LifelongLearning/2014\_Wang\_transfer\_multiple\_instance.pdf:pdf},
isbn = {9781577356783},
journal = {Proceedings of 28th AAAI Conference on Artificial Intelligence},
title = {{Adaptive Knowledge Transfer for Multiple Instance Learning in Image Classification}},
year = {2014}
}
